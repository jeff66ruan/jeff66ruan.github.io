<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Linux | For Fun and Logging]]></title>
  <link href="http://jeff66ruan.github.io/blog/categories/linux/atom.xml" rel="self"/>
  <link href="http://jeff66ruan.github.io/"/>
  <updated>2014-04-01T09:55:41+08:00</updated>
  <id>http://jeff66ruan.github.io/</id>
  <author>
    <name><![CDATA[Jeff Ruan]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[SigPnd, ShdPnd, SigBlk, SigIgn and SigCgt Fields in Proc Information File System]]></title>
    <link href="http://jeff66ruan.github.io/blog/2014/03/31/sigpnd-sigblk-sigign-sigcgt-in-proc-status-file/"/>
    <updated>2014-03-31T20:59:51+08:00</updated>
    <id>http://jeff66ruan.github.io/blog/2014/03/31/sigpnd-sigblk-sigign-sigcgt-in-proc-status-file</id>
    <content type="html"><![CDATA[<p>The following is the field definition from Linux man proc(5).</p>

<blockquote><p>SigPnd, ShdPnd: Number of signals pending for thread and for process as a whole (see pthreads(7) and signal(7)).</p>

<p>SigBlk, SigIgn, SigCgt: Masks indicating signals being blocked, ignored, and caught (see signal(7)).</p></blockquote>

<p>But what they look like and how to interpret? Here is an output example:</p>

<blockquote><p>SigPnd: 0000000000000000
ShdPnd: 0000000000000000
SigBlk: fffffffe7ffbfeff
SigIgn: 0000000000000000
SigCgt: 0000000000000000</p></blockquote>

<p>They are displayed in hex format as 8 bytes. The right most 4 bytes represent stardard signals, the left is Linux real-time signal extension. Each bit in the 8 bytes represents one corresponding signal. If the bits index starts from zero(the right most bit in the above), The corresponding signal is represented by bit[signalValue-1]. An example is that the signal SIGHUP, whose value is 1, is represented the bit 0.</p>

<p>Take the above SigBlk as example, the first two bytes are</p>

<blockquote><p>0xfeff</p></blockquote>

<p>The binary format is</p>

<blockquote><p>1111,1110,1111,1111</p></blockquote>

<p>It means all signals from 1 to 16 are blocked except the signal 9 (SIGKILL). This is true because SIGKILL cannot be blocked or ignored.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Oprofile and Memory Cache - an Experiment]]></title>
    <link href="http://jeff66ruan.github.io/blog/2014/03/24/oprofile-and-cache-an-experiment/"/>
    <updated>2014-03-24T11:03:38+08:00</updated>
    <id>http://jeff66ruan.github.io/blog/2014/03/24/oprofile-and-cache-an-experiment</id>
    <content type="html"><![CDATA[<p>Last week, I did a small experiment of using oprofile to profile two piece of code. In this small experiment, I learned some basic ideas of using oprofile. In particular, I used oprofile to profile memory cache behaviour and the performance effect. In this experiment, I got ideas of how memory cache invalidation can affect the speed of running program. The following is the detail.</p>

<h3>The Experiment Configuration</h3>

<p>I ran the experiment in an Thinkpad T400 with a Ubuntu installation. The experiment needs to run on a multi-core processor because of profiling memory cache invalidation cost. In a single core processor, there is no such memory cache invalidation cost.</p>

<p>```
$sudo lsb_release -a
No LSB modules are available.
Distributor ID: Ubuntu
Description:    Ubuntu 12.04 LTS
Release:        12.04
Codename:       precise</p>

<p>$ cat /proc/cpuinfo |grep &ldquo;model name&rdquo;
model name      : Intel&reg; Core&trade;2 Duo CPU     P8600  @ 2.40GHz
model name      : Intel&reg; Core&trade;2 Duo CPU     P8600  @ 2.40GHz
```</p>

<h3>Source Code</h3>

<p>There are two pieces of programs, which are &ldquo;no_alignment&rdquo; and &ldquo;alignment&rdquo;. They are compiled with GNU GCC. Each program clones a child sharing the same memory address space, which means the parent and the child updating different fileds of the same global data. The difference is that the program &ldquo;alignment&rdquo; optimizing the fields of the shared_data to the cache line size alignment. In this way, the two fields are able to be fetched on different cache lines. Therefore, when the parent and the child runs on different cores, the &ldquo;no_alignment&rdquo; program has the cache invlidation costs between two cores, but the &ldquo;alignment&rdquo; program doesn&rsquo;t.</p>

<p>The following are the source codes of the two programs. The only difference is the definition of struct shared_data.</p>

<p>``` c no_alignment</p>

<h1>define _GNU_SOURCE</h1>

<h1>include &lt;sched.h></h1>

<h1>include &lt;stdio.h></h1>

<h1>include &lt;errno.h></h1>

<h1>include &lt;stdlib.h></h1>

<p>// shared data
struct shared_data {
  unsigned int num_proc1;
  unsigned int num_proc2;
};</p>

<p>struct shared_data shared;</p>

<p>// define loops to run for a while
int loop_i = 100000, loop_j = 100000;</p>

<p>int child(void *);</p>

<h1>define STACK_SIZE (8 * 1024)</h1>

<p>int main(void)
{
  pid_t pid;
  int i, j;</p>

<p>  /<em> Stack </em>/
  char <em>stack = (char </em>)malloc(STACK_SIZE);
  if (!stack) {</p>

<pre><code>perror("malloc");
exit(1);
</code></pre>

<p>  }</p>

<p>  printf(&ldquo;main: shared %p %p\n&rdquo;,</p>

<pre><code>     &amp;shared.num_proc1, &amp;shared.num_proc2);
</code></pre>

<p>  /<em> clone a thread sharing memory space with the parent process </em>/
  if ((pid = clone(child, stack + STACK_SIZE, CLONE_VM, NULL)) &lt; 0) {</p>

<pre><code>perror("clone");
exit(1);
</code></pre>

<p>  }</p>

<p>  for (i = 0; i &lt; loop_i; i++) {</p>

<pre><code>for(j = 0; j &lt; loop_j; j++) {
  shared.num_proc1++;
}
</code></pre>

<p>  }
}</p>

<p>int child(void *arg)
{
  int i, j;</p>

<p>  printf(&ldquo;child: shared %p %p\n&rdquo;,</p>

<pre><code>     &amp;shared.num_proc1, &amp;shared.num_proc2);
</code></pre>

<p>  for (i = 0; i &lt; loop_i; i++) {</p>

<pre><code>for (j = 0; j &lt; loop_j; j++) {
  shared.num_proc2++;
}
</code></pre>

<p>  }
}
```</p>

<p>``` c alignment</p>

<h1>define _GNU_SOURCE</h1>

<h1>include &lt;sched.h></h1>

<h1>include &lt;stdio.h></h1>

<h1>include &lt;errno.h></h1>

<h1>include &lt;stdlib.h></h1>

<p>// cache line size
// hardware dependent value
// it can checks from /proc/cpuinfo</p>

<h1>define CACHE_LINE_SIZE 64</h1>

<p>// shared data aligned with cache line size
struct shared_data {
  unsigned int <strong>attribute</strong> ((aligned (CACHE_LINE_SIZE))) num_proc1;
  unsigned int <strong>attribute</strong> ((aligned (CACHE_LINE_SIZE))) num_proc2;
};</p>

<p>struct shared_data shared;</p>

<p>// define loops to run for a while
int loop_i = 100000, loop_j = 100000;</p>

<p>int child(void *);</p>

<h1>define STACK_SIZE (8 * 1024)</h1>

<p>int main(void)
{
  pid_t pid;
  int i, j;</p>

<p>  /<em> Stack </em>/
  char <em>stack = (char </em>)malloc(STACK_SIZE);
  if (!stack) {</p>

<pre><code>perror("malloc");
exit(1);
</code></pre>

<p>  }</p>

<p>  printf(&ldquo;main: shared %p %p\n&rdquo;,</p>

<pre><code>     &amp;shared.num_proc1, &amp;shared.num_proc2);
</code></pre>

<p>  /<em> clone a thread sharing memory space with the parent process </em>/
  if ((pid = clone(child, stack + STACK_SIZE, CLONE_VM, NULL)) &lt; 0) {</p>

<pre><code>perror("clone");
exit(1);
</code></pre>

<p>  }</p>

<p>  for (i = 0; i &lt; loop_i; i++) {</p>

<pre><code>for(j = 0; j &lt; loop_j; j++) {
  shared.num_proc1++;
}
</code></pre>

<p>  }
}</p>

<p>int child(void *arg)
{
  int i, j;</p>

<p>  printf(&ldquo;child: shared %p %p\n&rdquo;,</p>

<pre><code>     &amp;shared.num_proc1, &amp;shared.num_proc2);
</code></pre>

<p>  for (i = 0; i &lt; loop_i; i++) {</p>

<pre><code>for (j = 0; j &lt; loop_j; j++) {
  shared.num_proc2++;
}
</code></pre>

<p>  }
}</p>

<p>```</p>

<h3>Testing</h3>

<p>I was using the Oprofile 0.99 which has &lsquo;operf&rsquo; program that allows non-root users to profile a specified individual process with less setup. Different CPUs have different supported events. The supported events, their meanings and accepted event format by operf can be checked from ophelp and the CPU&rsquo;s hardware manual. In this experiment, the CLOCK event is CPU_CLK_UNHALTED and L2_LINES_IN is the number of allocated lines in L2 which is L2 cache missing number. As examples, the &ldquo;CPU_CLK_UNHALTED:100000&rdquo; tells operf to sample every 100000 unhalted clock with default mask(unhalted core cycles). The &ldquo;L2_LINES_IN:100000:0xf0&rdquo; tells operf to sample every 1000000 number of allocated lines in L2 on all cores with all prefetch inclusive.</p>

<p>The following is the experiment results.</p>

<p>``` bash profiling result of alignment
$time operf &mdash;event=CPU_CLK_UNHALTED:100000,L2_LINES_IN:100000:0xf0 ./alignment</p>

<p>operf: Profiler started
main: shared 0x6010c0 0x601100
child: shared 0x6010c0 0x601100</p>

<p>profiled app exited with the following status: 160</p>

<p>Profiling done.</p>

<p>real    0m41.373s
user    0m54.015s
sys     0m0.728s</p>

<p>$opreport
Using /home/work/oprof_cache/oprofile_data/samples/ for samples directory.
CPU: Core 2, speed 2.401e+06 MHz (estimated)
Counted CPU_CLK_UNHALTED events (Clock cycles when not halted) with a unit mask of 0x00 (Unhalted core cycles) count 100000
Counted L2_LINES_IN events (number of allocated lines in L2) with a unit mask of 0xf0 (multiple flags) count 100000
CPU_CLK_UNHALT&hellip;|L2_LINES_IN:10&hellip;|</p>

<h2>  samples|      %|  samples|      %|</h2>

<p>   939093 100.000    396933 100.000 alignment</p>

<pre><code>    CPU_CLK_UNHALT...|L2_LINES_IN:10...|
      samples|      %|  samples|      %|
    ------------------------------------
       936430 99.7164    395920 99.7448 alignment
         2657  0.2829      1013  0.2552 no-vmlinux
            5 5.3e-04         0       0 ld-2.15.so
            1 1.1e-04         0       0 libc-2.15.so
</code></pre>

<p>```</p>

<p>``` bash profiling result of no_alignment
$time operf &mdash;event=CPU_CLK_UNHALTED:100000,L2_LINES_IN:100000:0xf0 ./no_alignment
operf: Profiler started
main: shared 0x601058 0x60105c
child: shared 0x601058 0x60105c
profiled app exited with the following status: 160</p>

<p>Profiling done.</p>

<p>real    1m24.739s
user    1m59.167s
sys     0m1.344s</p>

<p>$opreport
Using /home/work/oprof_cache/oprofile_data/samples/ for samples directory.
CPU: Core 2, speed 2.401e+06 MHz (estimated)
Counted CPU_CLK_UNHALTED events (Clock cycles when not halted) with a unit mask of 0x00 (Unhalted core cycles) count 100000
Counted L2_LINES_IN events (number of allocated lines in L2) with a unit mask of 0xf0 (multiple flags) count 100000
CPU_CLK_UNHALT&hellip;|L2_LINES_IN:10&hellip;|</p>

<h2>  samples|      %|  samples|      %|</h2>

<p>  1423030 100.000   1177262 100.000 no_alignment</p>

<pre><code>    CPU_CLK_UNHALT...|L2_LINES_IN:10...|
      samples|      %|  samples|      %|
    ------------------------------------
      1419249 99.7343   1174038 99.7261 no_alignment
         3778  0.2655      3224  0.2739 no-vmlinux
            2 1.4e-04         0       0 ld-2.15.so
            1 7.0e-05         0       0 libc-2.15.so
</code></pre>

<p>```</p>

<h3>Analysis and Conclusion</h3>

<p>I drawed the table for the analysising.</p>

<p><img src="media/Oprofile-and-Cache/Oprofile-and-Cache.png"></p>

<p>Here is the conclusion:</p>

<ul>
<li>The cache missing rate of no_alignment was 82.73%. It became 42.27% with alignment optimization. The cache missing rate was reduced to almost half.</li>
<li>The no_alignment ran 120.5 seconds and the alignment ran 55 seconds. It was a bit more than double faster with alignment optimization.</li>
</ul>


<blockquote><p><strong>NOTE:</strong> The real time is less than user + sys because the program runs on mutlecores parallel</p></blockquote>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[双向链表在Linux Kernel中的实现]]></title>
    <link href="http://jeff66ruan.github.io/blog/2014/02/27/doubly-linked-list-in-the-linux-kernel/"/>
    <updated>2014-02-27T05:41:58+08:00</updated>
    <id>http://jeff66ruan.github.io/blog/2014/02/27/doubly-linked-list-in-the-linux-kernel</id>
    <content type="html"><![CDATA[<p>双向链表是一个简单的数据结构,使用非常广泛，应该人人知道，会用，打开编辑器马上可以写一个。但是Linux的双向链表实现非常巧妙，它使用了一个小小的编程技巧，使得开发人员定义自己的数据结构时添加一个struct list_head字段（list_head是Linux Kernel定义的双向链表的数据结构），就可以使自己的数据结构具备双向链表功能，采用Linux Kernel提供的操作函数，使得使用双向链表非常简单。如果我们在v3.9-rc8中grep list_head，竟然有多达11400多条匹配，可见双向链表在Linux kernel中的江湖地位，实际上，我们在读Linux源码时，到处可以见到以list_head组织起来的数据结构。</p>

<p><img src="media/Doubly-Linked-List-in-the-Linux-Kernel/lines-of-list-head.png"></p>

<p>下面我们将详细看看list_head结构及其相关函数，并且用一个简单例子学习如何使用它们。</p>

<h4>list_head结构</h4>

<p>数据结构是代码的核心，我们先来看看双向链表的数据结构。linux kernel在&lt;linux/list.h>中定义了双向链表的数据结构struct list_head。list_head定义很简单，只有两个成员函数next和prev，用于指向list_head：</p>

<p>``` c struct list_head的定义
struct list_head {</p>

<pre><code>   struct list_head *next, *prev;
</code></pre>

<p>};
```</p>

<h4>函数列表</h4>

<p>在list_head定义的基础上，Linux kernel提供了丰富的链表操作函数，下表列出了其中一些主要的。所有双向链表的操作都应该使用操作函数，而不应该直接访问list_head内部的next和prev指针，这样当list_head的实现改变时，使用的代码可以不受到影响。</p>

<p><img src="media/Doubly-Linked-List-in-the-Linux-Kernel/list-operations.png"></p>

<h4>使用</h4>

<p>Linux kernel的双向列表实现巧妙，只要在我们的数据结构中定义一个struct list_head的字段就可以使我们的数据结构具备双向链表的功能。通过链表提供的操作函数，我们可以创建自己的双向链表。一个创建好的链表为下图所示的组织形式：</p>

<p><img src="media/Doubly-Linked-List-in-the-Linux-Kernel/list-head-general.png"></p>

<p>我们可以通过一个例子来具体的理解和体会这个结构的方便简单的使用方式。</p>

<h4>例子</h4>

<h5>定义自己的数据结构</h5>

<p>定义我们的struct my_struct_t结构：</p>

<p><code>c 定义my_struct_t
typedef struct my_struct {
  int id;
  struct list_head lists;
  char *name;
} my_struct_t;
</code></p>

<p>struct list_head可以在数据结构中任意位置。上面代码中，我们定义了一个my_struct_t的结构，有三个字段id, lists和name。其中lists申明为struct list_head.</p>

<h5>定义链表头</h5>

<p><code>c 定义链表头my_struct_list
LIST_HEAD(my_struct_list);
</code></p>

<p>LIST_HEAD是一个宏，它实际上声明了一个struct list_head变量my_struct_list，并且初始化字段next和prev指向自身。初始化完成之后，my_struct_list实际为如下图所示：</p>

<p><img src="media/Doubly-Linked-List-in-the-Linux-Kernel/list-head-zero.png"></p>

<h5>插入一个链表项</h5>

<p>我们可以用list_add(new, head)向在链表头部插入一个链表项。</p>

<p>``` c 插入my_struct2到链表
my_struct_t my_struct2 = {.id = 2,</p>

<pre><code>                      .name = "struct2"};
</code></pre>

<p>list_add(&amp;my_struct2.lists, &amp;my_struct_list);
```</p>

<p>插入my_struct2之后，我们看到的链表结构如下图所示：</p>

<p><img src="media/Doubly-Linked-List-in-the-Linux-Kernel/list-head-one.png"></p>

<h5>再插入一个链表项</h5>

<p>``` c 插入my_struct1到链表
my_struct_t my_struct1 = {.id = 1,</p>

<pre><code>                      .name = "struct1"};
</code></pre>

<p>list_add(&amp;my_struct1.lists, &amp;my_struct_list);
```</p>

<p>插入my_struct1之后，我们看到的链表结构如下图所示：</p>

<p><img src="media/Doubly-Linked-List-in-the-Linux-Kernel/list-head-two.png"></p>

<h5>从链表项指针得到原数据结构的指针</h5>

<p>从链表项指针到原数据结构的指针是双向链表使用便利的关键，如果我们得到了一个链表项，可以通过list_entry(ptr, type, member)来得到指向struct list_head嵌入的数据结构的指针。其中ptr是一个指向struct list_head的指针，type是struct list_head嵌入的数据结构，memeber是struct list_head嵌入的字段名。看一个例子会容易明白：</p>

<p><code>c 得到指向原数据结构的指针
my_struct_t *my_structp = list_entry(my_struct_list.next, my_struct_t, lists);
</code></p>

<p>my_structp会指向上述链表结构中my_struct1，和下述赋值语句等效</p>

<p><code>c
my_structp = &amp;my_struct1;
</code></p>

<p>大家可能对list_entry如何实现比较好奇，下面是简化的源代码，使用了c中一个常见的技巧：</p>

<p>``` c list_entry的实现</p>

<h1>define list_entry(ptr, type, member) \</h1>

<pre><code>      (type *) ((char *) ptr - offset(type, memeber);
</code></pre>

<h1>define offsetof(TYPE, MEMBER) ((size_t) &amp;((TYPE *)0)&ndash;>MEMBER)</h1>

<p>```</p>

<p>offsetof计算字段MEMBER在类型TYPE中的偏移，这个偏移在编译的时候就由编译器计算得到。list_entry将链表项的指针减去list_head字段的偏移就得到了原数据结构的指针。</p>

<h5>遍历整个链表list_for_each和list_for_each_entry</h5>

<p>使用list_for_each(pos, head)可以遍历整个链表的链表项，head是链表头，pos是指向当前链表项的指针，每次循环时，pos都会更新为下一个链表项，下面例子打印列表中的id和name：</p>

<p>``` c 遍历一
struct list_head <em>pos;
my_struct_t </em>my_structp;</p>

<p>list_for_each(pos, &amp;my_struct_list) {
  my_structp = list_entry(pos, my_struct_t, lists);
  printf(&ldquo;id=%d, name=%s\n&rdquo;, my_structp->id, my_structp->name);
}
```</p>

<p>输出</p>

<blockquote><p>id=1, name=struct1
id=2, name=struct2</p></blockquote>

<p>list_for_each_entry(pos, head, member)可以遍历整个链表的原数据结构，head是链表头，pos为指向当前链表项所嵌入的原数据结构的指针，member是被嵌入数据结构struct list_head申明的字段名。实际上是list_for_each和list_entry的组合，所以上段代码也可以写成下面的方式，看起来更加简洁：</p>

<p>``` c 遍历二
my_struct_t *my_structp;</p>

<p>list_for_each_entry(my_structp, &amp;my_struct_list, lists) {
  printf(&ldquo;id=%d, name=%s\n&rdquo;, my_structp->id, my_structp-name);
}
```</p>

<h4>总结</h4>

<ol>
<li>使用双向链表需要在自己的数据结构中申明一个struct list_head的字段，定义的数据结构通过这个字段链接起来</li>
<li>使用LIST_HEAD宏申明一个链表头，这个链表头指向双向链表</li>
<li>使用&lt;linux/list.h>提供的add，delete等操作函数管理链表</li>
<li>通过list_entry及其他_entry结尾的函数得到原数据结构的指针，list_entry的实现采用了一个小技巧，是链表方便易用的原因</li>
</ol>

]]></content>
  </entry>
  
</feed>
